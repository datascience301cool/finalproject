---
title: "Final Project Long Report"
subtitle: "Data Science III (STAT 301-3)"
author: "Preston Chan, Edwin Chalas, Lauren Caldrone, and Josh Levitas"
date: "June 9^th^, 2021"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    highlight: "tango"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
# Load packages
library(tidyverse)
library(tidymodels)
library(janitor)
library(patchwork)
library(splitstackshape)

# Set seed here!
set.seed(57)
```

## Introduction

For our final project, we decided to analyze a data set of used cars listed on Craigslist called the "Used Car Dataset." This data set was compiled by Austin Reese and was downloaded from Kaggle. It contains the prices of used cars listed on Craigslist, an online marketplace that has the world's largest collection of used cars for sale. The dataset includes all used car listings in the United States, and it was last updated in December 2020.

Before cleaning the data, there were about 458,000 rows, each row representing a unique listing on Craigslist. There were 26 columns, each describing a specific characteristic of each car such as price, condition, manufacturer, type, etc.

The goal of this project was to develop a model that can accurately predict the price of a used car on Craigslist based on a set of its characteristics. The ultimate research question we are trying to answer is "how can we best predict the price of used cars?". This is a predictive question with price as the outcome variable, so oue research question is answered using a regression-based model. The motivation behind answering this question is that having an accurate model will be very useful in the real world, especially for people who are shopping for used cars. This is because a used car shopper could use the model to predict what the price should be for a car they are interested in. This would prevent them from purchasing overpriced cars and also help them identify cars that may be undervalued based on their characteristics.

Throughout the project, we refer to the data set as the `vehicles` data set. The data was read in using the following code. Since we will be demonstrating the data cleaning we performed, we read in the unprocessed data.

```{r, message=FALSE, warning=FALSE, eval = FALSE}
vehicles <- read_csv("data/unprocessed/vehicles.csv") %>% 
  clean_names()
```
```{r, message = FALSE, echo = FALSE}
load('data/vehicles.rda')
```


## EDA and Data Cleaning

Before beginning the EDA, we simplified the data set by removing irrelevant columns These columns include things like the URL to the listing, the listing ID, the url to the listing's image, etc. These columns will have no predictive power.

```{r, eval=FALSE}
vehicles <- vehicles %>% 
  select(-c(id, url, region_url, vin, image_url, description, posting_date))
```

After doing this, we began the EDA by performing an assurance check of the data using `skimr::skim_without_charts()`. 

```{r}
skimr::skim_without_charts(vehicles)
```

The main takeaway from the results was that a large handful of the variables had missingness, which was not surprising considering the massive size of the data set. Some of the variables that seemed to be important predictors (based on my intuition) had a somewhat high proportion of missingness, namely `condition`, `cylinders`, `drive`, `size`, and `type`. The proportion of missing values in these columns was 42%, 37%, 41%, 29%, 70%, and 25%, respectively. Most of the other columns had very low missingness. Another thing we noticed was that `state` had 51 unique vales and `region` had 405 unique values. We knew that this would cause an issue later on when creating a recipe and turning these variables into dummy variables since it would produce far too many columns, so we  decided to classify the states into 5 regions: `northeast`, `southeast`, `midwest`, `southwest`, and `west`.

```{r}
vehicles <- vehicles %>% 
  mutate(state_region = 
           case_when(
             state %in% c("wv", "va", "ky", "nc", "sc", "tn", "ar", "la", "al", "ms", "ga", "fl") ~ "southeast", 
             state %in% c("me", "vt", "nh", "ma", "ct", "ri", "ny", "nj", "pa", "de", "md", "dc") ~ "northeast", 
             state %in% c("nd", "sd", "mn", "wi", "ia", "ne", "ks", "mo", "il", "in", "mi", "oh") ~ "midwest", 
             state %in% c("ok", "tx", "nm", "az") ~ "southwest", 
             state %in% c("ak", "hi", "wa", "or", "id", "mt", "wy", "co", "ut", "nv", "ca") ~ "west"
           )
  )
```

We also noticed that many variables that should be classified as factor variables were classified as character variables. We easily fixed this with the following code.

```{r}
vehicles <- vehicles %>% 
  mutate(condition = factor(condition)) %>% 
  mutate(cylinders = factor(cylinders)) %>% 
  mutate(fuel = factor(fuel)) %>% 
  mutate(title_status = factor(title_status)) %>% 
  mutate(transmission = factor(transmission)) %>% 
  mutate(drive = factor(drive)) %>% 
  mutate(size = factor(size)) %>% 
  mutate(type = factor(type)) %>% 
  mutate(paint_color = factor(paint_color)) %>% 
  mutate(state_region = factor(state_region)) %>% 
  mutate(manufacturer = factor(manufacturer))
```

With the basic data cleaning done, we proceeded to conduct an EDA on the data. We started by focusing on the outcome variable, `price`. We first inspected its distribution.

```{r, message=FALSE, warning=FALSE}
plot1 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_freqpoly() + 
  labs(title = "Frequency Polygon")
plot2 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_histogram() + 
  labs(title = "Histogram")
plot3 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_density() + 
  labs(title = "Density Plot")
plot4 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_boxplot() + 
  labs(title = "Box Plot")
(plot1 + plot2) / (plot3 + plot4)
```

Based on these plots, it was difficult to see the distribution because the distribution was so skewed to the right. We decided to see if we could filter out any potential outliers to make it easier to see the distribution. We first looked at how many vehicles were priced at over $100,000, since this seemed like a price would be considered an outlier among used cars.

```{r}
vehicles %>% 
  filter(price > 100000) %>% 
  count()
```

Only 596 vehicles out of over 458,000 listings were priced at above $100,000. This implied that cars listed for more than $100,000 were clearly outliers. 

We also decided to look at how many listings were priced at below $1,000. This is because earlier, we  had gone on Craiglist to look at cars priced below $1,000 and discovered that many of these listings were for cars that could be paid off in monthly increments of a few hundred dollars. Since these monthly prices do not reflect the full value of the car, we decided it would be a good idea to filter these out. We decided to see how many listings would be taken out by this filter.

```{r}
vehicles %>% 
  filter(price < 1000) %>% 
  count()
```

More than 48,000 listings fell into this price range. This seemed like a high number, so we decided to look further into this subset of the data. We looked at the number of cars priced at $0.

```{r}
vehicles %>% 
  filter(price == 0) %>% 
  count()
```

More than 33,000 listings were priced at $0. Upon looking at such listings on Craigslist, we  discovered that almost all of these listings were from people who were hiding the real price of the car. In other words, they set the price to $0, and then inside the description, they would include a link to a different website which had the actual price of the car.

Based on these analyses, We decided that listings priced below $1,000 or above $100,000 were outliers. Thus, we looked at the distribution of `price` again after filtering out these outliers.

```{r, message=FALSE, warning=FALSE}
plot1 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  ggplot(aes(x = price)) + 
  geom_freqpoly() + 
  labs(title = "Frequency Polygon")
plot2 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  ggplot(aes(x = price)) + 
  geom_histogram() + 
  labs(title = "Histogram")
plot3 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  ggplot(aes(x = price)) + 
  geom_density() + 
  labs(title = "Density Plot")
plot4 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  ggplot(aes(x = price)) + 
  geom_boxplot() + 
  labs(title = "Box Plot")
(plot1 + plot2) / (plot3 + plot4)
```

The distribution was easier to see than before, but it was still skewed to the right. Because of this, we decided to log transform `price` and look at the distribution again.

```{r, message=FALSE, warning=FALSE}
plot1 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_freqpoly() + 
  labs(title = "Frequency Polygon")
plot2 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_histogram() + 
  labs(title = "Histogram")
plot3 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_density() + 
  labs(title = "Density Plot")
plot4 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_boxplot() + 
  labs(title = "Box Plot")
(plot1 + plot2) / (plot3 + plot4)
```

This distribution looked much more normal, and the boxplot showed no outliers. Therefore, we decided to filter out the outliers from the `vehicles` data set and log transform `price`.

```{r}
vehicles <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price))
```

The next thing we wanted to inspect was the correlation of the numeric x-variables (`year`, `odometer`, `lat`, and `long`) with `price` to see if any of them would be useful predictors that could be included in the model recipes. We used `stats::cor()` to create a correlation table for these variables, making sure to filter out listings with `price` greater than $1000 and and `price` less than $100,000.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  select(price, year, odometer, lat, long) %>% 
  cor(use = "complete.obs")
```

From these results it was apparent that the only variable that had a correlation with `price` (albeit a small one) was year. The other variables essentially had no correlation. However, we wanted to see if there were unusual values in these variables that were decreasing their correlation with `price`. In particular, we  wanted to look for unusual values in `year` and `odometer` as there cannot be unusual values for `lat` (latitude) and `long` (longitude) since they are simply geographical coordinates.

We decided to first look at the distribution of `year` to see if there were any unusual values affecting its correlation with `price`.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  ggplot(aes(year)) + 
  geom_boxplot() + 
  labs(title = "Distribution of Year")
```

From the boxplot, it was clear that there were many outliers, and these outliers were from years prior to roughly 1993.

We then looked at the distribution of `odometer` to see if there were any unusual values affective its correlation with `price`.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  ggplot(aes(odometer)) + 
  geom_boxplot() + 
  labs(title = "Distribution of Odometer")
```

From the boxplot, it was clear that there were a few extremely large outliers, which made it difficult to see if there were less extreme outliers. However, we  knew that most cars start to fall apart when they reach 250,000-300,000 miles, so we  decided to see how many cars in the data set had over 300,000 miles on their `odometer`.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  filter(odometer > 300000) %>% 
  count()
```

Only 2,154 listings out of the 400,000+ listings had an odometer with over 300,000 miles. Therefore, we  considered these as outliers.

Having found the unusual values in `year` and `odometer` that may have been decreasing their correlation with `price`, we  created a new correlation table in which we  filtered out the unusual values.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  filter(odometer < 300000) %>% 
  filter(year > 1993) %>% 
  select(price, year, odometer, lat, long) %>% 
  cor(use = "complete.obs")
```

Based on these results, it was clear that `year` and `odometer` actualy have moderate correlations with `price`. In other words, they seemed like useful predictors that could be included in the model recipes.

Furthermore, the EDA revealed that `year` and `odometer` had notable outliers that should be removed from the `vehicles` data set. Thus, we  used the following code to remove these outliers from `vehicles`.

```{r}
vehicles <- vehicles %>% 
  filter(odometer < 300000) %>% 
  filter(year > 1993)
```

The last step in the data processing was shrinking the size of the data set. We were forced to do this because we  initially tried tuning my models using the full data set, and by the time we  had waited about 10 hours, the tuning process was still not finished. We decided to take a stratified sample (stratified by `price`) equivalent to 5% the size of the cleaned `vehicles` data set.

```{r, eval=FALSE}
vehicles <- vehicles %>% 
  stratified("price", 0.05)
```

```{r, echo=FALSE}
load("data/vehicles_strat.rda")
```

This was the final "processed" data set that we  used for the modeling process.


## Modeling

### Data Spending

With the data cleaning, EDA, and data processing done, we  proceeded to begin the modeling process by first splitting the data. We split the data into the training and testing sets, allocating 70% to the training set and 30% to the testing set.

```{r}
set.seed(57)
vehicle_split <- initial_split(vehicles, prop = 0.7, strata = price)
vehicle_train <- training(vehicle_split)
vehicle_test <- testing(vehicle_split)
```

Next, we  compared the dimensions (rows and columns) of the training and testing sets to the full data set. We were able to confirm that the dimensions matched.

```{r}
dim(vehicle_split)
dim(vehicle_train)
dim(vehicle_test)
```

Finally, we  created resamples using v-fold cross-validation, folding the training set using 10 folds and 5 repeats and stratifying by `price`.

```{r}
model_folds <- vfold_cv(vehicle_train, v = 5, repeats = 3, strata = price)
```

### Modeling Setup

To set up everything we needed for the modeling process, we started by creating a recipe for our random forest, k-nearest neighbors, boosted tree, and support vector machine models. We chose to include numeric variables (`year` and `odometer`) and categorical variables (`manufacturer`, `fuel`, `title_status`, `transmission`). We chose to leave out `model` because this variable had 2,951 unique values, which would create far too many columns when being dummy encoded. We left out `region` (with 389 unique values) and `state` (with 51 unique values) for the same reason. We also left out `paint_color` since it had 12 unique values and did not seem like a strong predictor of price. We left out `lat` and `long` because they had no correlation with `price` (which was discovered in the EDA). Finally, we left out `size`, `condition`, `cylinders`, `drive`, and `type` because they all had greater than 20% missingness.

For imputation, we used `step_medianimpute()` for the numeric variables and `step_modeimpute()` for the categorical variables. Previously, we tried using `step_knnimpute()` for all variables, but it seemed to cause a lot of errors. We used `step_other()` on all categorical variables to pool infrequently occurring values into an "other" category. This helped a lot with reducing data sparseness as well as the total number of variables in the prepped and baked recipe (which made model tuning much smoother). We dummy encoded all nominal variables using one-hot encoding. We used `step_zv()` to remove variables that only contain a single value. Finally, we used `step_normalize()` to center and scale all predictors.

```{r}
# Recipe
vehicle_recipe <- recipe(price ~ year + manufacturer + fuel + odometer + title_status + transmission, data = vehicle_train) %>% 
  step_impute_median(year, odometer) %>% 
  step_impute_mode(manufacturer, fuel, title_status, transmission) %>% 
  step_other(all_nominal(), -transmission, -fuel) %>% 
  step_dummy(all_nominal(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric(), -all_outcomes())
```

Next, we prepped and baked both recipes.

```{r}
# Prep and bake the recipe
baked_data <- vehicle_recipe %>% 
  prep(vehicle_train) %>% 
  bake(new_data = NULL)
```

Finally, we set up the models for the random forest, k-nearest neighbors, boosted tree, and support vector machine models, being sure to tune `mtry` and `min_n` for the random forest model, `neighbors` for the k-nearest neighbors model, `sample_size` for the boosted tree model, and `cost` and `rbf_sigma` for the support vector machine model.

```{r}
# Random forest
rf_model <- rand_forest(
  mode = "regression", 
  mtry = tune(), 
  min_n = tune()
) %>% 
  set_engine("ranger")

# K-nearest neighbors
knn_model <- nearest_neighbor(
  mode = "regression", 
  neighbors = tune()
) %>% 
  set_engine("kknn")

# Boosted tree
btree <- boost_tree(
  mode = 'regression',
  sample_size = tune()
) %>%
  set_engine('xgboost')

# Support vector machin
svmrb_model <- svm_rbf(
  mode = "regression",
  cost = tune(),
  rbf_sigma = tune()
) %>%
  set_engine("kernlab")
```

### Tuning Setup

Next, we set up the tuning parameters for the random forest and k-nearest neighbors models. Starting with the random forest model, we set a custom range for `mtry` since a default is not given. The range we chose was from 2 to 17 variables. We set the minimum to 2 because we wanted at least 2 predictors to be used, and we set the maximum to 17 because there were 17 total perdictors in the prepped and baked recipe. We set the number of levels in the grid to 5 to again balance complexity with computational efficiency.

```{r}
rf_params <- parameters(rf_model) %>% 
  update(mtry = mtry(range = c(2, 17)))

rf_grid <- grid_regular(rf_params, levels = 5)
```

For the k-nearest neighbors model, we did not have to customize `min_n` because there was already a good default. For the grid, we again set the number of levels to 5.

```{r}
knn_params <- parameters(knn_model)

knn_grid <- grid_regular(knn_params, levels = 5)
```

For the boosted tree model, we did not have to customize `sample_size` because there was already a good default. For the grid, we again set the number of levels to 5.

```{r}
params <- parameters(btree)

btree_grid <- grid_regular(params, levels = 5)
```

For the support vector machine model, we did not have to customize `cost` or `rbf_sigma` because there were already good defaults for both. For the grid, we again set the number of levels to 5.

```{r}
svmrb_param <- parameters(svmrb_model) 

svmrb_grid <- grid_regular(svmrb_param, levels = 5)
```

Finally, we set up the workflows for each model using `vehicle_recipe`.

```{r}
# Random forest
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(vehicle_recipe)

# K-nearest neighbors
knn_workflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(vehicle_recipe)

# Boosted tree
btree_wflow <- workflow() %>%
  add_model(btree) %>%
  add_recipe(vehicle_recipe)

# Support vector machine
svmrb_workflow <- workflow() %>% 
  add_model(svmrb_model) %>% 
  add_recipe(vehicle_recipe)
```

### Tuning

Finally, we tuned the random forest, k-nearest neighbors, boosted tree, and support vector machine models, setting the resamples equal to `model_folds`, the resamples created earlier using v-fold cross-validation.

```{r, eval=FALSE}
# Random forest
rf_tune <- rf_workflow %>% 
  tune_grid(
    resamples = model_folds, 
    grid = rf_grid
  )

# K-nearest neighbors
knn_tune <- knn_workflow %>% 
  tune_grid(
    resamples = model_folds, 
    grid = knn_grid
  )

# Boosted tree
btree_tune <- btree_wflow %>%
  tune_grid(
    resamples = model_folds,
    grid = btree_grid
  )

# Support vector machine
svmrb_tuned <- svmrb_workflow %>% 
  tune_grid(
    resamples = model_folds, 
    grid = svmrb_grid
  )
```

```{r, echo=FALSE}
# Random forest
load(file = "data/rf_tune.rda")

# K-nearest neighbors
load(file = "data/knn_tune.rda")

# Boosted tree
load(file = "data/btree_tune.rda")

# Support vector machine
load(file = "data/svmrb_tuned.rda")
```

### Evaluation of results

To evaluate the tuning results for the random forest and k-nearest neighbors models, we used `autoplot()` to view the change in RMSE as the parameters changed.

```{r}
# Random forest
rf_tune %>% 
  autoplot(metric = "rmse")

# K-nearest neighbors
knn_tune %>% 
  autoplot(metric = "rmse")

# Boosted tree
btree_tune %>% 
  autoplot(metric = "rmse")

# Support vector machine
svmrb_tuned %>% 
  autoplot(metric = "rmse")
```

Based on `autoplot()` for the random forest model, between 2 and 5 `mtry`s, the RMSE dropped sharply for all values of `min_n`. Between 5 and 9 `mtry`s, the RMSE decreased for larger values of `min_n` (21, 30, and 40) but increased for smaller values of `min_n` (2 and 11). Beyond 9 `mtry`s, the RMSE increased for all values of `min_n`. It seemed to increase more for smaller values or `min_n`. It appears that the optimal parameters were 9 for `mtry` and 40 for `min_n`.

Based on the `autoplot()` for the k-nearest neighbors model, the RMSE strictly decreased as the number of `neighbors` increased, but it decreased at a diminishing rate. It appears that the optimal number of `neighbors` was 15.

Based on the `autoplot()` for the boosted tree model, the RMSE strictly decreased as the proportion of observations sampled (i.e., the `sample_size`) increased, but it decreased at a diminishing rate. It appears that the optimal `sample_size` was 1.

Based on the `autoplot()` for the support vector machine model, the RMSE seemed to generally decrease as `cost` increased. This is especially true for three values of `rbf_sigma` (1.0e-05, 1.0e+00, and 3.162278e-03), which all had pretty large decreases in RMSE. For the other two values of `rbf_sigma`, RMSE generally remained constant as `cost` increased. It appears that the optimal parameters were 32 for `cost` and 3.162278e-03 for `rbf_sigma`.

Next, we used `select_best()` on the random forest, k-nearest neighbors, boosted tree, and support vector machine models to verify that our judgements were correct about which parameters produced the lowest RMSE for each model.

```{r}
# Best random forest parameters
rf_tune %>% 
  select_best(metric = "rmse")

# Best k-nearest neighbors parameters
knn_tune %>% 
  select_best(metric = "rmse")

# Best boosted tree parameters
btree_tune %>% 
  select_best(metric = "rmse")

# Best support vector machine parameters
svmrb_tuned %>% 
  select_best(metric = "rmse")
```

Next, we compared the RMSEs produced by each of the three models (random forest, k-nearest neighbors, boosted tree, and support vector machine) to find the best overall model.

```{r}
tune_results <- tibble(
  model_type = c("rf", "knn", "btree", "svmrb"), 
  tune_info = list(rf_tune, knn_tune, btree_tune, svmrb_tuned), 
  assessment_info = map(tune_info, collect_metrics), 
  best_model = map(tune_info, ~ select_best(.x, metric = "rmse"))
)

tune_results %>% 
  select(model_type, assessment_info) %>% 
  unnest(assessment_info) %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean)
```

From these results, we were able to determine that the random forest model with an `mtry` of 9 and a `min_n` of 40 had the lowest RMSE at about 0.210. Having found the best model, we then created a workflow using this model and then fitted it to the training set.

```{r}
# Workflow
rf_workflow_tuned <- rf_workflow %>% 
  finalize_workflow(select_best(rf_tune, metric = "rmse"))

# Fitting to training set
rf_results <- fit(rf_workflow_tuned, vehicle_train)
```

Finally, we tested the fitted model on the testing set and compared the predicted `price`s to the actual `price`s to get the RMSE.

```{r, message=FALSE, warning=FALSE}
predict(rf_results, new_data = vehicle_test) %>% 
  bind_cols(vehicle_test %>% select(price)) %>% 
  rmse(truth = price, estimate = .pred)
```

The RMSE was about 0.211, meaning that the model performed slightly worse on the testing data than it did on the resamples.


## Conclusion and Next Steps

Overall, the random forest model seemed to be pretty accurate in predicting the `price` of a used car on Craigslist given its `year`, `manufacturer`, `fuel`, `odometer`, `title_status`, `transmission`.

We were curious about how the model would do at predicting the price of a real used car listing on Craigslist. We found a specific listing, a 2005 Ford F-150 XLT, created a data frame containing the car's characteristics, and used this data frame and the random forest model to predict the `price` of the car.

```{r}
# Create data frame for real listing

used_car_listing <- tibble(
  year = 2005, 
  manufacturer = "ford", 
  condition = "good", 
  cylinders = "8 cylinders", 
  fuel = "gas", 
  odometer = 130222, 
  title_status = "clean", 
  transmission = "automatic", 
  drive = "4wd", 
  type = "truck", 
  state_region = "midwest"
)

# Predict price
ex_1 <- predict(rf_results, used_car_listing)

10^ex_1
```

Undoing the log transformation for this truck, the predicted price was $5,428. This was not far off the actual listing price of $5,200. It is possible that this particular listing was underpriced, or that the model overestimated the price. 

Given that this particular listing's `price` was on low side, also wanted to try predicting the `price` of a more expensive listing. We found a listing for a 2018 Jeep Grand Cherokee listed for $29,650. Again, we created a data frame containing the truck's characteristics and used the data frame and the random forest model to predict the `price` of the truck.

```{r}
# Create data frame for real listing

used_car_listing2 <- tibble(
  year = 2018, 
  manufacturer = "jeep", 
  condition = "excellent", 
  cylinders = "6 cylinders", 
  fuel = "gas", 
  odometer = 33894, 
  title_status = "clean", 
  transmission = "automatic", 
  drive = "4wd", 
  type = "SUV", 
  state_region = "midwest"
)

# Predict price
ex_2 <- predict(rf_results, used_car_listing2)

10^ex_2
```

Undoing the log transformation, the predicted `price` ended up being $28,966, which is really close to the actual `price`. We're pretty satisfied with these results, but seeing that the model overestimated the `price` in the first test and then underestimated it in the second test, we think that the model could be improved.

As for next steps, seeing the random forest model best three others makes us confident that this would be the best way to go for accuracy - especially looking at the closeness of the results given real data. However, we may want to try adjusting some tuning parameters to make this model even more accurate. 

Ultimately, we're quite satisfied with the predictive accuracy of the random forest model given the amount of time we had to work on the project. We believe it could be a useful tool for used car shoppers who are trying to assess the pricing of a particular listing. However, if given more time, our next steps would be to try new tuning parameters to get an even lower RMSE.


## Citations

Used Car Dataset: https://www.kaggle.com/austinreese/craigslist-carstrucks-data


## GitHub Repo Link

Link: https://github.com/datascience301cool/finalproject.git

