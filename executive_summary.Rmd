---
title: "301-3 Final Project Executive Summary"
subtitle: "Data Science II (STAT 301-2)"
author: "Lauren, Preston, Edwin, Josh"
date: "June 9^th^, 2021"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    highlight: "tango"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
# Load packages
library(tidyverse)
library(tidymodels)
library(janitor)
library(patchwork)
library(splitstackshape)

# Read in unprocessed data
load(file = "data/vehicles.rda")

# Read in processed data
load(file = "data/vehicles_strat.rda")

# Split the data
set.seed(2021)

vehicles_strat <- vehicles %>% 
  stratified("price", 0.05)

vehicle_split <- vehicles_strat %>%
  initial_split(prop = .8, strata = price)

vehicle_train <- training(vehicle_split)

vehicle_test <- testing(vehicle_split)
```

## Executive Summary

For our final project, we decided to create four different predictive models that can predict the price of used cars based on a set of car characteristics. After creating all four models, we tested them to see which performed the best. The data set was based off of all used car listings in the U.S. on Craigslist.

When first looking at the data, our group saw that there was missingness in a lot of the variables, probably explained by how big the data set actually is. We simply took care of missingness in the EDA and recipe. Also, becuase 596 vehicles of over 458,000 listings were above $100,000, we considred them to be outliers. We also thought fitering out listings below $1,000 would be important because they do not reflect the full value of the car. 

```{r}
# looked at distribution of price
plot1 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_freqpoly() + 
  labs(title = "Frequency Polygon")

plot2 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_histogram() + 
  labs(title = "Histogram")

plot3 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_density() + 
  labs(title = "Density Plot")

plot4 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_boxplot() + 
  labs(title = "Box Plot")

(plot1 + plot2) / (plot3 + plot4)
```

To start the modeling process, we split the data into training and testing sets and created resamples using v-fold cross-validation. 

We then created a recipe forall four models including svrmb, bt, nn, and rf.

In the recipe, we chose to include numeric variables (`year` and `odometer`) and categorical variables (`manufacturer`, `fuel`, `title_status`, `transmission`). 

For imputation, we used `step_medianimpute()` for the numeric variables and `step_modeimpute()` for the categorical variables. When trying to use`step_knnimpute()` for all variables, errors came up. 

We dummy encoded all nominal variables and used one-hot encoding. Finally, we used `step_zv()` to remove variables that were highly sparse, which also helped eliminate missingness when tuning, and we used `step_normalize()` for all numeric.

```{r}
# Recipe for svmrb, bt, knnn, and rf
vehicles_recipe <- recipe(price ~ year + manufacturer + fuel + odometer + title_status + transmission, data = vehicle_train) %>% 
  step_medianimpute(year, odometer) %>% 
  step_modeimpute(manufacturer, fuel, title_status, transmission) %>% 
  step_other(all_nominal(), -transmission, -fuel) %>% 
  step_dummy(all_nominal(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric(), -all_outcomes())
```

After creating the recipes, we created workflows for each model and then tuned the random forest, nearest neighbors, boosted tree, and SVM radial basis models.

```{r, include=FALSE}
# Load random forest tuning
load(file = "data/rf_tune.rda")

# Load svmrb tuning
load(file = "data/svmrb_tuned.rda")

# Load boosted tree tuning
load(file = "data/btree_tune.rda")

# Load knn tune
load(file = "data/knn_tune")
```

We then evaluated the results. For all four models, we used `autoplot()` to view the change in RMSE as the parameters changed.

```{r}
# Evaluate results for random forest
rf_tune %>% 
  autoplot(metric = "rmse")

# Evaluate results for SVM radial basis
svmrb_tuned %>%
  autoplot(metric = 'rmse')

# Evaluate results for boosted tree
btree_tune %>%
  autoplot(metric = 'rmse')

# Evaluate results for k-nearest neighbors
knn_tune %>%
  autoplot(metric = 'rmse')
```

Based on `autoplot()` for the random forest model, between 2 and 5 `mtry`s, the RMSE dropped sharply for all values of `min_n`. Beyond 5 `mtry`s, the RMSE either stayed relatively the same or increased depending on the value of `min_n`. The optimal parameters were 9 for `mtry` and 40 for `min_n`.

Based on the `autoplot()` for the k-nearest neighbors model, the RMSE strictly decreased as the number of `neighbors` increased, but it decreased at a diminishing rate. The optimal number of `neighbors` was 15.

Based on the `autoplot()` for the boosted tree model, the RMSE strictly decreased as the proportion of observations sampled (i.e., the `sample_size`) increased, but it decreased at a diminishing rate. The optimal `sample_size` was 1.

Based on the `autoplot()` for the support vector machine model, the RMSE seemed to generally decrease as `cost` increased for certain values of `rbf_sigma`. For the other values of `rbf_sigma` the RMSE remained relatively constant as `cost` increased. The optimal parameters were 32 for `cost` and 3.162278e-03 for `rbf_sigma`.

To get the overall best model (as well as the parameters used by that model), we combined the results for all four models into one data frame and arranged the RMSE from smallest to largest.

```{r}
tune_results <- tibble(
  model_type = c("rf", "knn", "btree", "svmrb"), 
  tune_info = list(rf_tune, knn_tune, btree_tune, svmrb_tuned), 
  assessment_info = map(tune_info, collect_metrics), 
  best_model = map(tune_info, ~ select_best(.x, metric = "rmse"))
)

tune_results %>% 
  select(model_type, assessment_info) %>% 
  unnest(assessment_info) %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean)
```

The random forest model using 9 for `mtry` and 40 for `min_n` had the lowest RMSE at 0.210. With this being the best model, we fitted it to the training data and then tested it on the testing data.

```{r}
# Workflow
rf_workflow_tuned <- rf_workflow %>% 
  finalize_workflow(select_best(rf_tune, metric = "rmse"))

# Fitting to training set
rf_results <- fit(rf_workflow_tuned, vehicle_train)

# Testing
predict(rf_results, new_data = vehicle_test) %>% 
  bind_cols(vehicle_test %>% select(price)) %>% 
  rmse(truth = price, estimate = .pred)
```

The RMSE was about 0.211, meaning that the model performed slightly worse on the testing data than it did on the resamples.

After testing all models, the random forest model was the most accurate in predicting 'price' of a used car on Cragislist given its `year`, `manufacturer`, `fuel`, `odometer`, `title_status`, and `transmission`.

To get a better sense of the accuracy of the model, we decided to test it on an actual used car listing on Craigslist (specifically, a 2005 Ford F-150 XLT). We used this data frame and the random forest model to predict price. 

```{r}
# Create data frame for real listing
used_car_listing <- tibble(
  year = 2005, 
  manufacturer = "ford", 
  condition = "good", 
  cylinders = "8 cylinders", 
  fuel = "gas", 
  odometer = 130222, 
  title_status = "clean", 
  transmission = "automatic", 
  drive = "4wd", 
  type = "truck", 
  state_region = "midwest"
)

# Predict price
ex_1 <- predict(rf_results, used_car_listing)

10^ex_1
```

After undoing the log transformation, we found that the predicted price was $5,428, which was not far off the actual listing price of $5,200. Either the car was overpriced or the model underestimated the price.

Given that this particular listing's `price` was on low side, we also wanted to try predicting the `price` of a more expensive listing. we found a listing for a 2018 Jeep Grand Cherokee listed for $29,650.

```{r}
# Create data frame for real listing
used_car_listing2 <- tibble(
  year = 2018, 
  manufacturer = "jeep", 
  condition = "excellent", 
  cylinders = "6 cylinders", 
  fuel = "gas", 
  odometer = 33894, 
  title_status = "clean", 
  transmission = "automatic", 
  drive = "4wd", 
  type = "SUV", 
  state_region = "midwest"
)

# Predict price
ex_2 <- predict(rf_results, used_car_listing2)

10^ex_2
```

After undoing the log transformation, the predicted `price` ended up being $28,966, which was also pretty close to the actual `price`. We were  satisfied with these results; however, after seeing that the model underestimated the `price` in the first test and then overestimated it in the second test, we knew that the there was room for improvement for this model.

In the end, we were satisfied with the predictive accuracy of the random forest model. We believe it could be a useful tool for used car shoppers who are trying to assess the pricing of a particular listing. With more time, our next steps would be to try new models and/or tuning parameters to get an even lower RMSE, which would hopefully improve the predictive accuracy. 
