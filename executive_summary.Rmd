---
title: "301-3 Final Project Executive Summary"
subtitle: "Data Science III (STAT 301-3)"
author: "Preston Chan, Edwin Chalas, Lauren Caldrone, and Josh Levitas"
date: "June 9^th^, 2021"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    highlight: "tango"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
# Load packages
library(tidyverse)
library(tidymodels)
library(janitor)
library(patchwork)
library(splitstackshape)

# Read in unprocessed data
load(file = "data/unprocessed/vehicles.rda")

# Read in processed data
load(file = "data/processed/vehicles_strat.rda")

# Split the data
set.seed(57)
vehicle_split <- initial_split(vehicles_strat, prop = 0.8, strata = price)
vehicle_train <- training(vehicle_split)
vehicle_test <- testing(vehicle_split)
```

## Executive Summary

For our final project, we decided to create four different predictive models that can predict the price of used cars based on a set of car characteristics. After creating all four models, we tested them to see which performed the best. The data set was based off of all used car listings in the U.S. on Craigslist.

When first looking at the data, our group saw that there was missingness in a lot of the variables, probably explained by how big the data set actually is. We simply took care of missingness in the EDA and recipe. Also, because 97 vehicles of 111,860 listings were above $100,000, we considered them to be outliers. We also thought filtered out listings priced under $1,000 because earlier, we had gone on Craiglist to look at cars priced below $1,000 and discovered that many of these listings were for cars that could be paid off in monthly increments of a few hundred dollars, and these monthly prices do not reflect the full value of the car. We also performed a log transformation of `price` to normalize the distribution. The graphs below show the distribution of `price` before and after the above changes were made to the dataset.

```{r}
# looked at distribution of price
hist_before <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_histogram() + 
  labs(title = "Before") + 
  theme(plot.title = element_text(hjust = 0.5))
box_before <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_boxplot()
hist_after <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_histogram() + 
  labs(title = "After") + 
  theme(plot.title = element_text(hjust = 0.5))
box_after <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_boxplot()
(hist_before + hist_after) / (box_before + box_after)
```

To start the modeling process, we split the data into training and testing sets and created resamples using v-fold cross-validation. 

We then created a recipe for all four models including random forest, k-nearest neighbors, boosted tree, and support vector machine with radial basis function.

In the recipe, we chose to include numeric variables (`year` and `odometer`) and categorical variables (`manufacturer`, `fuel`, `title_status`, and `transmission`). 

For imputation, we used `step_impute_median()` for the numeric variables and `step_impute_mode()` for the categorical variables. Previously, we tried using `step_knnimpute()` for all variables, but it seemed to cause a lot of errors. We used `step_other()` on all categorical variables to pool infrequently occurring values into an "other" category. This helped a lot with reducing data sparseness as well as the total number of variables in the prepped and baked recipe (which made model tuning much smoother). We dummy encoded all nominal variables using one-hot encoding. We used `step_zv()` to remove variables that only contain a single value. Finally, we used `step_normalize()` to center and scale all predictors.

```{r}
# Recipe for rf, knn, btree, and svmrb
vehicle_recipe <- recipe(price ~ year + manufacturer + fuel + odometer + title_status + transmission, data = vehicle_train) %>% 
  step_impute_median(year, odometer) %>% 
  step_impute_mode(manufacturer, fuel, title_status, transmission) %>% 
  step_other(all_nominal(), -transmission, -fuel) %>% 
  step_dummy(all_nominal(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric(), -all_outcomes())
```

After creating the recipe, we created workflows for each model and then tuned the random forest, k-nearest neighbors, boosted tree, and SVM radial basis models.

```{r, include=FALSE}
# Load random forest tuning
load(file = "tuning/data/rf_tune.rda")

# Load svmrb tuning
load(file = "tuning/data/svmrb_tuned.rda")

# Load boosted tree tuning
load(file = "tuning/data/btree_tune.rda")

# Load knn tune
load(file = "tuning/data/knn_tune.rda")
```

We then evaluated the results. For all four models, we used `autoplot()` to view the change in RMSE as the parameters changed.

```{r}
# Evaluate results for random forest
rf_tune %>% 
  autoplot(metric = "rmse")

# Evaluate results for SVM radial basis
svmrb_tuned %>%
  autoplot(metric = 'rmse')

# Evaluate results for boosted tree
btree_tune %>%
  autoplot(metric = 'rmse')

# Evaluate results for k-nearest neighbors
knn_tune %>%
  autoplot(metric = 'rmse')
```

Based on `autoplot()` for the random forest model, between 2 and 5 `mtry`s, the RMSE dropped sharply for all values of `min_n`. Beyond 5 `mtry`s, the RMSE either stayed relatively the same or increased depending on the value of `min_n`. The optimal parameters were 9 for `mtry` and 40 for `min_n`.

Based on the `autoplot()` for the k-nearest neighbors model, the RMSE strictly decreased as the number of `neighbors` increased, but it decreased at a diminishing rate. The optimal number of `neighbors` was 15.

Based on the `autoplot()` for the boosted tree model, the RMSE strictly decreased as the proportion of observations sampled (i.e., the `sample_size`) increased, but it decreased at a diminishing rate. The optimal `sample_size` was 1.

Based on the `autoplot()` for the support vector machine model, the RMSE seemed to generally decrease as `cost` increased for certain values of `rbf_sigma`. For the other values of `rbf_sigma` the RMSE remained relatively constant as `cost` increased. The optimal parameters were 32 for `cost` and 3.162278e-03 for `rbf_sigma`.

To get the overall best model (as well as the parameters used by that model), we combined the results for all four models into one data frame and arranged the RMSE from smallest to largest.

```{r}
tune_results <- tibble(
  model_type = c("rf", "knn", "btree", "svmrb"), 
  tune_info = list(rf_tune, knn_tune, btree_tune, svmrb_tuned), 
  assessment_info = map(tune_info, collect_metrics), 
  best_model = map(tune_info, ~ select_best(.x, metric = "rmse"))
)

tune_results %>% 
  select(model_type, assessment_info) %>% 
  unnest(assessment_info) %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean)
```

The random forest model using 9 for `mtry` and 40 for `min_n` had the lowest RMSE at 0.210. With this being the best model, we fitted it to the training data and then tested it on the testing data.

```{r}
# Workflow
rf_workflow_tuned <- rf_workflow %>% 
  finalize_workflow(select_best(rf_tune, metric = "rmse"))

# Fitting to training set
rf_results <- fit(rf_workflow_tuned, vehicle_train)

# Testing
predict(rf_results, new_data = vehicle_test) %>% 
  bind_cols(vehicle_test %>% select(price)) %>% 
  rmse(truth = price, estimate = .pred)
```

The RMSE was about 0.211, meaning that the model performed slightly worse on the testing data than it did on the resamples.

After testing all models, the random forest model was the most accurate in predicting the `price` of a used car on Cragislist given its `year`, `manufacturer`, `fuel`, `odometer`, `title_status`, and `transmission`.

To get a better sense of the accuracy of the model, we decided to test it on an actual used car listing on Craigslist (specifically, a 2005 Ford F-150 XLT). We used this data frame and the random forest model to predict price. 

```{r}
# Create data frame for real listing
used_car_listing <- tibble(
  year = 2005, 
  manufacturer = "ford", 
  fuel = "gas", 
  odometer = 130222, 
  title_status = "clean", 
  transmission = "automatic"
)

# Predict price
ex_1 <- predict(rf_results, used_car_listing)

10^ex_1
```

After undoing the log transformation, we found that the predicted price was $5,831, which was not far off the actual listing price of $5,200. Either the car was underpriced or the model overestimated the `price`.

Given that this particular listing's `price` was on low side, we also wanted to try predicting the `price` of a more expensive listing. We found a listing for a 2018 Jeep Grand Cherokee listed for $29,650.

```{r}
# Create data frame for real listing
used_car_listing2 <- tibble(
  year = 2018, 
  manufacturer = "jeep", 
  fuel = "gas", 
  odometer = 33894, 
  title_status = "clean", 
  transmission = "automatic"
)

# Predict price
ex_2 <- predict(rf_results, used_car_listing2)

10^ex_2
```

After undoing the log transformation, the predicted `price` ended up being $30,009, which was very close to the actual `price`. We were  satisfied with these results; however, after seeing that the model just barely over-predicted the `price` in both tests, we knew that the there was room for improvement for this model.

In the end, we were satisfied with the predictive accuracy of the random forest model. We believe it could be a useful tool for used car shoppers who are trying to assess the pricing of a particular listing. With more time, our next steps would be to try new models and/or tuning parameters to get an even lower RMSE, which would hopefully improve the predictive accuracy. 
