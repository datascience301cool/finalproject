---
title: "Data Memo"
author: "Edwin Chalas Cuevas, Preston Chan, Lauren Caldrone, Joshua Levitas"
date: "4/21/2021"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    highlight: "tango"
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages
library(tidyverse)
library(tidymodels)
library(janitor)
library(patchwork)
library(splitstackshape)

# Set seed here!
set.seed(57)
```

## Read In the Data

```{r}
vehicles <- read_csv("data/vehicles.csv") %>% 
  clean_names()
```

## EDA

Before beginning the EDA, I simplified the data set by removing irrelevant columns These columns include things likethe URL to the listing, the listing ID, the url to the listing's image, etc. These columns will have no predictive power.

```{r}
vehicles <- vehicles %>% 
  select(-c(x1, id, url, region_url, vin, image_url, description, posting_date))
```

After doing this, I began the EDA by performing an assurance check of the data using `skimr::skim_without_charts()`. 

```{r}
skimr::skim_without_charts(vehicles)
```

The main takeaway from the results was that a large handful of the variables had missingness, which was not surprising considering the massive size of the data set. Some of the variables that seemed to be important predictors (based on my intuition) had a somewhat high proportion of missingness, namely `condition`, `cylinders`, `drive`, `size`, and `type`. The proportion of missing values in these columns was 42%, 37%, 41%, 29%, 70%, and 25%, respectively. Most of the other columns had very low missingness. Another thing I noticed was that `state` had 51 unique vales and `region` had 405 unique values. I knew that this would cause an issue later on when creating a recipe and turning these variables into dummy variables since it would produce far too many columns, so I decided to classify the states into 5 regions: `northeast`, `southeast`, `midwest`, `southwest`, and `west`.

```{r}
vehicles <- vehicles %>% 
  mutate(state_region = 
           case_when(
             state %in% c("wv", "va", "ky", "nc", "sc", "tn", "ar", "la", "al", "ms", "ga", "fl") ~ "southeast", 
             state %in% c("me", "vt", "nh", "ma", "ct", "ri", "ny", "nj", "pa", "de", "md", "dc") ~ "northeast", 
             state %in% c("nd", "sd", "mn", "wi", "ia", "ne", "ks", "mo", "il", "in", "mi", "oh") ~ "midwest", 
             state %in% c("ok", "tx", "nm", "az") ~ "southwest", 
             state %in% c("ak", "hi", "wa", "or", "id", "mt", "wy", "co", "ut", "nv", "ca") ~ "west"
           )
  )
```

I also noticed that many variables that should be classified as factor variables were classified as character variables. I easily fixed this with the following code.

```{r}
vehicles <- vehicles %>% 
  mutate(condition = factor(condition)) %>% 
  mutate(cylinders = factor(cylinders)) %>% 
  mutate(fuel = factor(fuel)) %>% 
  mutate(title_status = factor(title_status)) %>% 
  mutate(transmission = factor(transmission)) %>% 
  mutate(drive = factor(drive)) %>% 
  mutate(size = factor(size)) %>% 
  mutate(type = factor(type)) %>% 
  mutate(paint_color = factor(paint_color)) %>% 
  mutate(state_region = factor(state_region)) %>% 
  mutate(manufacturer = factor(manufacturer))
```

With the basic data cleaning done, I proceeded to conduct an EDA on the data. I started by focusing on the outcome variable, `price`. I first inspected its distribution.

```{r, message=FALSE, warning=FALSE}
plot1 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_freqpoly() + 
  labs(title = "Frequency Polygon")
plot2 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_histogram() + 
  labs(title = "Histogram")
plot3 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_density() + 
  labs(title = "Density Plot")
plot4 <- vehicles %>% 
  ggplot(aes(x = price)) + 
  geom_boxplot() + 
  labs(title = "Box Plot")
(plot1 + plot2) / (plot3 + plot4)
```

Based on these plots, it was difficult to see the distribution because the distribution was so skewed to the right. I decided to see if I could filter out any potential outliers to make it easier to see the distribution. I first looked at how many vehicles were priced at over $100,000, since this seemed like a price would be considered an outlier among used cars.

```{r}
vehicles %>% 
  filter(price > 100000) %>% 
  count()
```

Only 596 vehicles out of over 458,000 listings were priced at above $100,000. This implied that cars listed for more than $100,000 were clearly outliers. 

I also decided to look at how many listings were priced at below $1,000. This is because earlier, I had gone on Craiglist to look at cars priced below $1,000 and discovered that many of these listings were for cars that could be paid off in monthly increments of a few hundred dollars. Since these monthly prices do not reflect the full value of the car, I decided it would be a good idea to filter these out. I decided to see how many listings would be taken out by this filter.

```{r}
vehicles %>% 
  filter(price < 1000) %>% 
  count()
```

More than 48,000 listings fell into this price range. This seemed like a high number, so I decided to look further into this subset of the data. I looked at the number of cars priced at $0.

```{r}
vehicles %>% 
  filter(price == 0) %>% 
  count()
```

More than 33,000 listings were priced at $0. Upon looking at such listings on Craigslist, I discovered that almost all of these listings were from people who were hiding the real price of the car. In other words, they set the price to $0, and then inside the description, they would include a link to a different website which had the actual price of the car.

Based on these analyses, I decided that listings priced below $1,000 or above $100,000 were outliers. Thus, I looked at the distribution of `price` again after filtering out these outliers.

```{r, message=FALSE, warning=FALSE}
plot1 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  ggplot(aes(x = price)) + 
  geom_freqpoly() + 
  labs(title = "Frequency Polygon")
plot2 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  ggplot(aes(x = price)) + 
  geom_histogram() + 
  labs(title = "Histogram")
plot3 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  ggplot(aes(x = price)) + 
  geom_density() + 
  labs(title = "Density Plot")
plot4 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  ggplot(aes(x = price)) + 
  geom_boxplot() + 
  labs(title = "Box Plot")
(plot1 + plot2) / (plot3 + plot4)
```

The distribution was easier to see than before, but it was still skewed to the right. Because of this, I decided to log transform `price` and look at the distribution again.

```{r, message=FALSE, warning=FALSE}
plot1 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_freqpoly() + 
  labs(title = "Frequency Polygon")
plot2 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_histogram() + 
  labs(title = "Histogram")
plot3 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_density() + 
  labs(title = "Density Plot")
plot4 <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price)) %>% 
  ggplot(aes(x = price)) + 
  geom_boxplot() + 
  labs(title = "Box Plot")
(plot1 + plot2) / (plot3 + plot4)
```

This distribution looked much more normal, and the boxplot showed no outliers. Therefore, I decided to filter out the outliers from the `vehicles` data set and log transform `price`.

```{r}
vehicles <- vehicles %>% 
  filter(price > 1000 & price < 100000) %>% 
  mutate(price = log10(price))
```

The next thing I wanted to inspect was the correlation of the numeric x-variables (`year`, `odometer`, `lat`, and `long`) with `price` to see if any of them would be useful predictors that could be included in the model recipes. I used `stats::cor()` to create a correlation table for these variables, making sure to filter out listings with `price` greater than $1000 and and `price` less than $100,000.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  select(price, year, odometer, lat, long) %>% 
  cor(use = "complete.obs")
```

From these results it was apparent that the only variable that had a correlation with `price` (albeit a small one) was year. The other variables essentially had no correlation. However, I wanted to see if there were unusual values in these variables that were decreasing their correlation with `price`. In particular, I wanted to look for unusual values in `year` and `odometer` as there cannot be unusual values for `lat` (latitude) and `long` (longitude) since they are simply geographical coordinates.

I decided to first look at the distribution of `year` to see if there were any unusual values affecting its correlation with `price`.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  ggplot(aes(year)) + 
  geom_boxplot() + 
  labs(title = "Distribution of Year")
```

From the boxplot, it was clear that there were many outliers, and these outliers were from years prior to roughly 1993.

I then looked at the distribution of `odometer` to see if there were any unusual values affective its correlation with `price`.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  ggplot(aes(odometer)) + 
  geom_boxplot() + 
  labs(title = "Distribution of Odometer")
```

From the boxplot, it was clear that there were a few extremely large outliers, which made it difficult to see if there were less extreme outliers. However, I knew that most cars start to fall apart when they reach 250,000-300,000 miles, so I decided to see how many cars in the data set had over 300,000 miles on their `odometer`.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  filter(odometer > 300000) %>% 
  count()
```

Only 2,154 listings out of the 400,000+ listings had an odometer with over 300,000 miles. Therefore, I considered these as outliers.

Having found the unusual values in `year` and `odometer` that may have been decreasing their correlation with `price`, I created a new correlation table in which I filtered out the unusual values.

```{r}
vehicles %>% 
  mutate(price = (10 ^ price)) %>% 
  filter(odometer < 300000) %>% 
  filter(year > 1993) %>% 
  select(price, year, odometer, lat, long) %>% 
  cor(use = "complete.obs")
```

Based on these results, it was clear that `year` and `odometer` actualy have moderate correlations with `price`. In other words, they seemed like useful predictors that could be included in the model recipes.

Furthermore, the EDA revealed that `year` and `odometer` had notable outliers that should be removed from the `vehicles` data set. Thus, I used the following code to remove these outliers from `vehicles`.

```{r}
vehicles <- vehicles %>% 
  filter(odometer < 300000) %>% 
  filter(year > 1993)
```
